{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HEsPC-CVdhQs"},"outputs":[],"source":["!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/env-check.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4bA80xFd8iS"},"outputs":[],"source":["!bash rapidsai-csp-utils/colab/update_gcc.sh\n","import os\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9xkovk_d_Th"},"outputs":[],"source":["import condacolab\n","condacolab.install()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3I_aaG-TeAYQ"},"outputs":[],"source":["import condacolab\n","condacolab.check()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9KsEmuueARP"},"outputs":[],"source":["!python rapidsai-csp-utils/colab/install_rapids.py stable\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","os.environ['CONDA_PREFIX'] = '/usr/local'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyR_uhsIeAKv"},"outputs":[],"source":["VER = 6\n","\n","import pandas as pd, numpy as np\n","from tqdm.notebook import tqdm\n","import os, sys, pickle, glob, gc\n","from collections import Counter\n","import cudf, itertools\n","print('We will use RAPIDS version',cudf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFFzxTpzd_35"},"outputs":[],"source":["%%time\n","# CACHE FUNCTIONS\n","def read_file(f):\n","    return cudf.DataFrame( data_cache[f] )\n","def read_file_to_cache(f):\n","    df = pd.read_parquet(f)\n","    df.ts = (df.ts/1000).astype('int32')\n","    df['type'] = df['type'].map(type_labels).astype('int8')\n","    return df\n","\n","# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n","data_cache = {}\n","type_labels = {'clicks':0, 'carts':1, 'orders':2}\n","files = glob.glob('/content/drive/MyDrive/OTTO/LB_576_Dataset_Tony/*_parquet/*')\n","for f in files: data_cache[f] = read_file_to_cache(f)\n","\n","# CHUNK PARAMETERS\n","READ_CT = 5\n","CHUNK = int( np.ceil( len(files)/6 ))\n","print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLPNVuv1jfvH"},"outputs":[],"source":["%%time\n","type_weight = {0:1, 1:5, 2:4}\n","\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 4\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # =\u003e OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # =\u003e INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i\u003cb: df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            \n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n\u003c30].drop('n',axis=1)\n","            \n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()\u003c 24 * 60 * 60) \u0026 (df.aid_x != df.aid_y) ]\n","            \n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x \u003e= PART*SIZE)\u0026(df.aid_x \u003c (PART+1)*SIZE)]\n","            \n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n","            df['wgt'] = df.type_y.map(type_weight)\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            \n","            # COMBINE INNER CHUNKS\n","            if k==a: tmp2 = df\n","            else: tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","        \n","        print()\n","        \n","        # COMBINE OUTER CHUNKS\n","        if a==0: tmp = tmp2\n","        else: tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","\n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    \n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n\u003c15].drop('n',axis=1)\n","    \n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IZmb9PklqKB"},"outputs":[],"source":["%%time\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 1\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # =\u003e OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # =\u003e INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            \n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i\u003cb: df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            \n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n\u003c30].drop('n',axis=1)\n","            \n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()\u003c 14 * 24 * 60 * 60) \u0026 (df.aid_x != df.aid_y) ] # 14 DAYS\n","            \n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x \u003e= PART*SIZE)\u0026(df.aid_x \u003c (PART+1)*SIZE)]\n","            \n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n","            df['wgt'] = 1\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            \n","            # COMBINE INNER CHUNKS\n","            if k==a: tmp2 = df\n","            else: tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","\n","        print()\n","        \n","        # COMBINE OUTER CHUNKS\n","        if a==0: tmp = tmp2\n","        else: tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","\n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    \n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n\u003c15].drop('n',axis=1)\n","    \n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9x15ggIl7It"},"outputs":[],"source":["%%time\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 4\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # =\u003e OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # =\u003e INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i\u003cb: df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            \n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n\u003c30].drop('n',axis=1)\n","            \n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()\u003c 24 * 60 * 60) \u0026 (df.aid_x != df.aid_y) ]\n","            \n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x \u003e= PART*SIZE)\u0026(df.aid_x \u003c (PART+1)*SIZE)]\n","            \n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n","            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n","            # 1659304800 : minimum timestamp\n","            # 1662328791 : maximum timestamp\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            \n","            # COMBINE INNER CHUNKS\n","            if k==a: tmp2 = df\n","            else: tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","        print()\n","        \n","        # COMBINE OUTER CHUNKS\n","        if a==0: tmp = tmp2\n","        else: tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","\n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    \n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n\u003c20].drop('n',axis=1)\n","    \n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11HZtWOAmAcK"},"outputs":[],"source":["# FREE MEMORY\n","del data_cache, tmp\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EB9ZqNMDmBIa"},"outputs":[],"source":["def load_test():    \n","    dfs = []\n","    for e, chunk_file in enumerate(glob.glob('/content/drive/MyDrive/OTTO/LB_576_Dataset_Tony/*')):\n","        chunk = pd.read_parquet(chunk_file)\n","        chunk.ts = (chunk.ts/1000).astype('int32')\n","        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n","        dfs.append(chunk)\n","    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n","\n","test_df = load_test()\n","print('Test data has shape',test_df.shape)\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YuJd_grmF_B"},"outputs":[],"source":["%%time\n","def pqt_to_dict(df):\n","    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n","\n","# LOAD THREE CO-VISITATION MATRICES\n","top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n","\n","for k in range(1,DISK_PIECES): \n","    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n","\n","\n","top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n","\n","for k in range(1,DISK_PIECES): \n","    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n","\n","top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n","\n","# TOP CLICKS AND ORDERS IN TEST\n","#top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n","#top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n","\n","print('Here are size of our 3 co-visitation matrices:')\n","print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av0_uAQtmF8v"},"outputs":[],"source":["top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:20] \n","top_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:20]\n","top_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lY6jfCUOmJ4p"},"outputs":[],"source":["#type_weight_multipliers = {'clicks': 1, 'carts': 5, 'orders': 4}\n","type_weight_multipliers = {0: 1, 1: 5, 2: 4}\n","\n","def suggest_clicks(df):\n","    # USER HISTORY AIDS AND TYPES\n","    aids=df.aid.tolist()\n","    types = df.type.tolist()\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    # RERANK CANDIDATES USING WEIGHTS\n","    if len(unique_aids)\u003e=20:\n","        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n","        return sorted_aids\n","    # USE \"CLICKS\" CO-VISITATION MATRIX\n","    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n","    # RERANK CANDIDATES\n","    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n","    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n","    # USE TOP20 TEST CLICKS\n","    return result + list(top_clicks)[:20-len(result)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0ulYncWmF5w"},"outputs":[],"source":["def suggest_carts(df):\n","    # User history aids and types\n","    aids = df.aid.tolist()\n","    types = df.type.tolist()\n","    \n","    # UNIQUE AIDS AND UNIQUE BUYS\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]\n","    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n","    \n","    # Rerank candidates using weights\n","    if len(unique_aids) \u003e= 20:\n","        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        \n","        # Rerank based on repeat items and types of items\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        \n","        # Rerank candidates using\"top_20_carts\" co-visitation matrix\n","        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n","        for aid in aids2: aids_temp[aid] += 0.1\n","        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n","        return sorted_aids\n","    \n","    # Use \"cart order\" and \"clicks\" co-visitation matrices\n","    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n","    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n","    \n","    # RERANK CANDIDATES\n","    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(20) if aid2 not in unique_aids] \n","    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n","    \n","    # USE TOP20 TEST ORDERS\n","    return result + list(top_carts)[:20-len(result)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzPeYDcmmF2F"},"outputs":[],"source":["def suggest_buys(df):\n","    # USER HISTORY AIDS AND TYPES\n","    aids=df.aid.tolist()\n","    types = df.type.tolist()\n","    # UNIQUE AIDS AND UNIQUE BUYS\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    df = df.loc[(df['type']==1)|(df['type']==2)]\n","    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n","    # RERANK CANDIDATES USING WEIGHTS\n","    if len(unique_aids)\u003e=20:\n","        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n","        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","        for aid in aids3: aids_temp[aid] += 0.1\n","        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n","        return sorted_aids\n","    # USE \"CART ORDER\" CO-VISITATION MATRIX\n","    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n","    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n","    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","    # RERANK CANDIDATES\n","    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n","    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n","    # USE TOP20 TEST ORDERS\n","    return result + list(top_orders)[:20-len(result)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"N9lDzIB9mQJ9"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 10h 34min 32s, sys: 12min 24s, total: 10h 46min 56s\n","Wall time: 10h 29min 21s\n"]}],"source":["%%time\n","\n","pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n","    lambda x: suggest_clicks(x)\n",")\n","\n","pred_df_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n","    lambda x: suggest_carts(x)\n",")\n","\n","pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n","    lambda x: suggest_buys(x)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CXcc5_gsmQH4"},"outputs":[],"source":["clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n","orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n","carts_pred_df = pd.DataFrame(pred_df_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"U7dnITfomQFZ"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-dd3d65ad-d359-4b96-b364-7095869ad162\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003esession_type\u003c/th\u003e\n","      \u003cth\u003elabels\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e0_clicks\u003c/td\u003e\n","      \u003ctd\u003e543308 974651 1199474 1549618 442293 1760145 3...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e1_clicks\u003c/td\u003e\n","      \u003ctd\u003e105393 711125 215311 854637 1491172 1734061 49...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e2_clicks\u003c/td\u003e\n","      \u003ctd\u003e161269 1577398 672473 808782 477910 485582 160...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e3_clicks\u003c/td\u003e\n","      \u003ctd\u003e54857 1018433 925352 1261998 1712999 1638009 1...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e4_clicks\u003c/td\u003e\n","      \u003ctd\u003e479396 1081407 678521 758750 917213 1554752 38...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3d65ad-d359-4b96-b364-7095869ad162')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-dd3d65ad-d359-4b96-b364-7095869ad162 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dd3d65ad-d359-4b96-b364-7095869ad162');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["  session_type                                             labels\n","0     0_clicks  543308 974651 1199474 1549618 442293 1760145 3...\n","1     1_clicks  105393 711125 215311 854637 1491172 1734061 49...\n","2     2_clicks  161269 1577398 672473 808782 477910 485582 160...\n","3     3_clicks  54857 1018433 925352 1261998 1712999 1638009 1...\n","4     4_clicks  479396 1081407 678521 758750 917213 1554752 38..."]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n","pred_df.columns = [\"session_type\", \"labels\"]\n","pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n","pred_df.to_csv(\"submission.csv\", index=False)\n","pred_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH2n_TGDHS_G"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNOWvQDOO1CpPyZK2d+VR0M","machine_shape":"hm","mount_file_id":"1lu1Id4QYg6LCRXVj6z0AKyckNabYVwKp","name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}